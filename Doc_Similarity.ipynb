{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================================================\n",
    "#                                               IDS 566 - HOMEWORK 1\n",
    "#==========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================================================\n",
    "#                                       Importing all the required libraries\n",
    "#===========================================================================================================================\n",
    "\n",
    "#pip install nltk       #(UNCOMMENT - For First Run Only)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================================================\n",
    "#                                           Reading all the input text files \n",
    "#===========================================================================================================================\n",
    "\n",
    "file1 = open(\"UIC.txt\", \"r\", encoding=\"utf8\")\n",
    "uic = file1.read()\n",
    "file1.close()\n",
    "\n",
    "file2 = open(\"UIUC.txt\", \"r\", encoding=\"utf8\")\n",
    "uiuc = file2.read()\n",
    "file2.close()\n",
    "\n",
    "file3 = open(\"UIS.txt\", \"r\", encoding=\"utf8\")\n",
    "uis = file3.read()\n",
    "file3.close()\n",
    "\n",
    "file4 = open(\"MIT.txt\", \"r\", encoding=\"utf8\")\n",
    "mit = file4.read()\n",
    "file4.close()\n",
    "\n",
    "file5 = open(\"Stanford.txt\", \"r\", encoding=\"utf8\")\n",
    "stanford = file5.read()\n",
    "file5.close()\n",
    "\n",
    "file6 = open(\"Tesla.txt\", \"r\", encoding=\"utf8\")\n",
    "tesla = file6.read()\n",
    "file6.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================================================\n",
    "#                                   Begin of Data Cleaning Process \n",
    "#===========================================================================================================================\n",
    "\n",
    "\n",
    "#1. Removing Punctuations\n",
    "\n",
    "uic      = uic.translate(str.maketrans('', '', string.punctuation))\n",
    "uiuc     = uiuc.translate(str.maketrans('', '', string.punctuation))\n",
    "uis      = uis.translate(str.maketrans('', '', string.punctuation))\n",
    "mit      = mit.translate(str.maketrans('', '', string.punctuation))\n",
    "stanford = stanford.translate(str.maketrans('', '', string.punctuation))\n",
    "tesla    = tesla.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#2. Removing Apostrophe\n",
    "uic      = uic.replace(\"'\", \"\")\n",
    "uiuc     = uiuc.replace(\"'\", \"\")\n",
    "uis      = uis.replace(\"'\", \"\")\n",
    "mit      = mit.replace(\"'\", \"\")\n",
    "stanford = stanford.replace(\"'\", \"\")\n",
    "tesla    = tesla.replace(\"'\", \"\")\n",
    "\n",
    "#3. Converting all words to Lowercase\n",
    "\n",
    "uic      = uic.lower()\n",
    "uiuc     = uiuc.lower()\n",
    "uis      = uis.lower()\n",
    "mit      = mit.lower()\n",
    "stanford = stanford.lower()\n",
    "tesla    = tesla.lower()\n",
    "\n",
    "#4. Tokenization\n",
    "\n",
    "wstk = WhitespaceTokenizer() \n",
    "\n",
    "uic      = wstk.tokenize(uic)\n",
    "uiuc     = wstk.tokenize(uiuc)\n",
    "uis      = wstk.tokenize(uis)\n",
    "mit      = wstk.tokenize(mit)\n",
    "stanford = wstk.tokenize(stanford)\n",
    "tesla    = wstk.tokenize(tesla)\n",
    "\n",
    "#5. Removing Stop Words\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "uic      = [w for w in uic if not w in stop_words]\n",
    "uiuc     = [w for w in uiuc if not w in stop_words]\n",
    "uis      = [w for w in uis if not w in stop_words]\n",
    "mit      = [w for w in mit if not w in stop_words]\n",
    "stanford = [w for w in stanford if not w in stop_words]\n",
    "tesla    = [w for w in tesla if not w in stop_words]\n",
    "\n",
    "#6. Stemming words\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "uic      = [porter.stem(word) for word in uic]\n",
    "uiuc     = [porter.stem(word) for word in uiuc]\n",
    "uis      = [porter.stem(word) for word in uis]\n",
    "mit      = [porter.stem(word) for word in mit]\n",
    "stanford = [porter.stem(word) for word in stanford]\n",
    "tesla    = [porter.stem(word) for word in tesla]\n",
    "\n",
    "#===========================================================================================================================\n",
    "#                                      End of Data Cleaning Process \n",
    "#==========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity between MIT and UIC is 0.18972999035679847\n",
      "Jaccard similarity between Stanford and UIC is 0.263913195659783\n",
      "Jaccard similarity between Tesla and UIC is 0.18243243243243243\n",
      "Jaccard similarity between UIS and UIC is 0.5672131147540984\n",
      "Jaccard similarity between UIUC and UIC is 0.3111979166666667\n"
     ]
    }
   ],
   "source": [
    "#===========================================================================================================================\n",
    "#                                    Calculating Jaccard Similarity between the 6 input documents \n",
    "#===========================================================================================================================\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersect_l = list(set(query) & set(document))\n",
    "    union_l = list(set(query) or set(document))\n",
    "    \n",
    "    return len(intersect_l)/len(union_l)\n",
    "\n",
    "Jac_sim_mit_uic=jaccard_similarity(mit, uic)\n",
    "print(\"Jaccard similarity between MIT and UIC is \"+ str(Jac_sim_mit_uic))\n",
    "Jac_sim_Stan_uic=jaccard_similarity(stanford, uic)\n",
    "print(\"Jaccard similarity between Stanford and UIC is \"+ str(Jac_sim_Stan_uic))\n",
    "Jac_sim_Tes_uic=jaccard_similarity(tesla, uic)\n",
    "print(\"Jaccard similarity between Tesla and UIC is \"+ str(Jac_sim_Tes_uic))\n",
    "Jac_sim_uis_uic=jaccard_similarity(uis, uic)\n",
    "print(\"Jaccard similarity between UIS and UIC is \"+ str(Jac_sim_uis_uic))\n",
    "Jac_sim_Tes_uiuc=jaccard_similarity(uiuc, uic)\n",
    "print(\"Jaccard similarity between UIUC and UIC is \"+ str(Jac_sim_Tes_uiuc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.76679328 0.60162564 0.35858466 0.4763952  0.22582247]\n",
      " [0.76679328 1.         0.72802877 0.40390357 0.54561252 0.26283644]\n",
      " [0.60162564 0.72802877 1.         0.25467702 0.38466961 0.12680528]\n",
      " [0.35858466 0.40390357 0.25467702 1.         0.32399016 0.19735668]\n",
      " [0.4763952  0.54561252 0.38466961 0.32399016 1.         0.24465725]\n",
      " [0.22582247 0.26283644 0.12680528 0.19735668 0.24465725 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#===========================================================================================================================\n",
    "#                       Calculating Cosine Similarity based on only Term Frequency(TF)\n",
    "#===========================================================================================================================\n",
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "#Unlist the cleaned data obtained above for calculating cosine similarity\n",
    "uic      = ','.join(str(v) for v in uic)\n",
    "uiuc     = ','.join(str(v) for v in uiuc)\n",
    "uis      = ','.join(str(v) for v in uis)\n",
    "mit      = ','.join(str(v) for v in mit)\n",
    "stanford = ','.join(str(v) for v in stanford)\n",
    "tesla    = ','.join(str(v) for v in tesla)\n",
    "\n",
    "#Merge all the documents into one document\n",
    "doc = [uic,uiuc,uis,mit,stanford,tesla]\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix    = count_vectorizer.fit_transform(doc)\n",
    "\n",
    "# Convert Sparse Matrix TF(Term Frequency) to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix  = sparse_matrix.todense()\n",
    "tf = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=['uic', 'uiuc','uis','mit','stanford','tesla'])\n",
    "\n",
    "\n",
    "# Computing Cosine Similarity based on tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(tf, tf))\n",
    "\n",
    "#Below is the Cosine Similarity Matrix based on only Term frequency-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.64338228 0.46427893 0.2393719  0.29727417 0.12533567]\n",
      " [0.64338228 1.         0.59480755 0.28967765 0.36147868 0.15902449]\n",
      " [0.46427893 0.59480755 1.         0.16042576 0.23253759 0.06401812]\n",
      " [0.2393719  0.28967765 0.16042576 1.         0.18176578 0.09928483]\n",
      " [0.29727417 0.36147868 0.23253759 0.18176578 1.         0.11260325]\n",
      " [0.12533567 0.15902449 0.06401812 0.09928483 0.11260325 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#===========================================================================================================================\n",
    "#                             Calculating Cosine Similarity based on TF-IDF\n",
    "#===========================================================================================================================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#The below variable tfidf contains the TF-IDF values for each term of these documents\n",
    "tfidf = vectorizer.fit_transform(doc)\n",
    "words = vectorizer.get_feature_names()\n",
    "similarity_matrix = cosine_similarity(tfidf)\n",
    "\n",
    "print(similarity_matrix)\n",
    "\n",
    "#Below is the Cosine Similarity Matrix based on TF-IDF-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
